{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing your Parquet files\n",
    "directory_path = 'C:/Users/comma/Github/workspace/sas-access-samples/SAS Foundation/parquetdataset/'\n",
    "\n",
    "# Use glob to find all Parquet files in the directory\n",
    "parquet_file_paths = glob.glob(directory_path + '*.parquet')\n",
    "\n",
    "# Initialize an empty dictoionary to store the DataFrames\n",
    "dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode bytes to string if the value is a byte literal\n",
    "def decode_bytes(val):\n",
    "    if isinstance(val, bytes):\n",
    "        return val.decode('utf-8')\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the file paths\n",
    "for path in parquet_file_paths:\n",
    "    # Extract the base file name without the extension as the dictionary key\n",
    "    file_name = path.split('/')[-1].replace('.parquet', '')\n",
    "    \n",
    "    # Read each file into a DataFrame\n",
    "    df = pd.read_parquet(path)\n",
    "    \n",
    "    # Apply the decode_bytes function to each column using Series.map\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].map(decode_bytes)\n",
    "    \n",
    "    # Store the DataFrame in the dictionary with the file name as the key\n",
    "    dfs[file_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfs['parquetdataset\\\\samdat1']\n",
    "df2 = dfs['parquetdataset\\\\samdat2']\n",
    "df3 = dfs['parquetdataset\\\\samdat3']\n",
    "df4 = dfs['parquetdataset\\\\samdat4']\n",
    "df5 = dfs['parquetdataset\\\\samdat5']\n",
    "df6 = dfs['parquetdataset\\\\samdat6']\n",
    "df7 = dfs['parquetdataset\\\\samdat7']\n",
    "df8 = dfs['parquetdataset\\\\samdat8']\n",
    "df9 = dfs['parquetdataset\\\\samdat9']\n",
    "df10 = dfs['parquetdataset\\\\samdat10']\n",
    "df11 = dfs['parquetdataset\\\\samdat11']\n",
    "df12 = dfs['parquetdataset\\\\samdat12']\n",
    "df13 = dfs['parquetdataset\\\\samdat13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   FLIGHT    46 non-null     object        \n",
      " 1   DATES     46 non-null     datetime64[ns]\n",
      " 2   DEPART    46 non-null     float64       \n",
      " 3   ORIG      46 non-null     object        \n",
      " 4   DEST      46 non-null     object        \n",
      " 5   MILES     46 non-null     float64       \n",
      " 6   BOARDED   46 non-null     float64       \n",
      " 7   CAPACITY  46 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(4), object(3)\n",
      "memory usage: 3.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   FLIGHT    46 non-null     object        \n",
      " 1   DATES     46 non-null     datetime64[ns]\n",
      " 2   ORIG      46 non-null     object        \n",
      " 3   DEST      46 non-null     object        \n",
      " 4   DELAYCAT  46 non-null     object        \n",
      " 5   DESTYPE   46 non-null     object        \n",
      " 6   DELAY     46 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(5)\n",
      "memory usage: 2.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26 entries, 0 to 25\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   FLIGHT   26 non-null     object        \n",
      " 1   DATES    26 non-null     datetime64[ns]\n",
      " 2   DEST     26 non-null     object        \n",
      " 3   BOARDED  26 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(2)\n",
      "memory usage: 964.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156 entries, 0 to 155\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   FLIGHT  156 non-null    object        \n",
      " 1   DATES   156 non-null    datetime64[ns]\n",
      " 2   DEST    156 non-null    object        \n",
      " 3   IDNUM   156 non-null    object        \n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 5.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148 entries, 0 to 147\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   IDNUM    148 non-null    object        \n",
      " 1   SEX      148 non-null    object        \n",
      " 2   JOBCODE  148 non-null    object        \n",
      " 3   SALARY   148 non-null    float64       \n",
      " 4   BIRTH    148 non-null    datetime64[ns]\n",
      " 5   HIRED    148 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(1), object(3)\n",
      "memory usage: 7.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12 entries, 0 to 11\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   IDNUM    12 non-null     object        \n",
      " 1   SEX      12 non-null     object        \n",
      " 2   JOBCODE  12 non-null     object        \n",
      " 3   SALARY   12 non-null     float64       \n",
      " 4   BIRTH    12 non-null     datetime64[ns]\n",
      " 5   HIRED    12 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(1), object(3)\n",
      "memory usage: 708.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148 entries, 0 to 147\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   IDNUM   148 non-null    object\n",
      " 1   LNAME   148 non-null    object\n",
      " 2   FNAME   148 non-null    object\n",
      " 3   CITY    148 non-null    object\n",
      " 4   STATE   148 non-null    object\n",
      " 5   HPHONE  148 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 7.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   SUPID   19 non-null     object\n",
      " 1   STATE   19 non-null     object\n",
      " 2   JOBCAT  19 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 588.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17 entries, 0 to 16\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   INVNUM    17 non-null     float64       \n",
      " 1   BILLEDTO  17 non-null     object        \n",
      " 2   AMTBILL   17 non-null     float64       \n",
      " 3   COUNTRY   17 non-null     object        \n",
      " 4   AMTINUS   17 non-null     float64       \n",
      " 5   BILLEDBY  17 non-null     float64       \n",
      " 6   BILLEDON  17 non-null     datetime64[ns]\n",
      " 7   PAIDON    10 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(4), object(2)\n",
      "memory usage: 1.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   EMPID     22 non-null     float64       \n",
      " 1   HIREDATE  22 non-null     datetime64[ns]\n",
      " 2   SALARY    21 non-null     float64       \n",
      " 3   DEPT      22 non-null     object        \n",
      " 4   JOBCODE   22 non-null     float64       \n",
      " 5   GENDER    21 non-null     object        \n",
      " 6   BIRTHDTE  21 non-null     datetime64[ns]\n",
      " 7   LASTNAME  22 non-null     object        \n",
      " 8   FRSTNAME  22 non-null     object        \n",
      " 9   MIDNAME   22 non-null     object        \n",
      " 10  PHONE     21 non-null     object        \n",
      "dtypes: datetime64[ns](2), float64(3), object(6)\n",
      "memory usage: 2.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   CUSTOMER  21 non-null     object        \n",
      " 1   STATE     11 non-null     object        \n",
      " 2   ZIPCODE   15 non-null     object        \n",
      " 3   COUNTRY   20 non-null     object        \n",
      " 4   PHONE     21 non-null     object        \n",
      " 5   NAME      20 non-null     object        \n",
      " 6   CONTACT   19 non-null     object        \n",
      " 7   ADDRESS   20 non-null     object        \n",
      " 8   CITY      20 non-null     object        \n",
      " 9   FIRSTORD  21 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(9)\n",
      "memory usage: 1.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38 entries, 0 to 37\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   ORDERNUM  38 non-null     float64       \n",
      " 1   STOCKNUM  38 non-null     float64       \n",
      " 2   LENGTH    38 non-null     float64       \n",
      " 3   FABCHARG  24 non-null     float64       \n",
      " 4   SHIPTO    38 non-null     object        \n",
      " 5   DATEORD   38 non-null     datetime64[ns]\n",
      " 6   SHIPPED   15 non-null     datetime64[ns]\n",
      " 7   TAKENBY   38 non-null     float64       \n",
      " 8   PROCSBY   15 non-null     float64       \n",
      " 9   SPECFLAG  10 non-null     object        \n",
      "dtypes: datetime64[ns](2), float64(6), object(2)\n",
      "memory usage: 3.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   EMPID     6 non-null      float64       \n",
      " 1   HIREDATE  6 non-null      datetime64[ns]\n",
      " 2   DEPT      6 non-null      object        \n",
      " 3   GENDER    5 non-null      object        \n",
      " 4   LASTNAME  6 non-null      object        \n",
      " 5   FIRSTNAM  6 non-null      object        \n",
      " 6   MIDDLENA  5 non-null      object        \n",
      " 7   FAMILYID  2 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(5)\n",
      "memory usage: 516.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df1.info()\n",
    "df2.info()\n",
    "df3.info()\n",
    "df4.info()\n",
    "df5.info()\n",
    "df6.info()\n",
    "df7.info()\n",
    "df8.info()\n",
    "df9.info()\n",
    "df10.info()\n",
    "df11.info()\n",
    "df12.info()\n",
    "df13.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 1       */\n",
    " /*=========================*/\n",
    "\n",
    "proc print data=samples.SAMDAT7\n",
    "   (keep=lname fname state hphone);\n",
    "   where state = 'NJ';\n",
    "   title 'Libname Sample 1: New Jersey Phone List';\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief Summary of SAS Program\n",
    "The SAS program is designed to display a subset of data from a dataset SAMDAT7 which is part of the samples library. It filters the data to only include records where the state is 'New Jersey' (NJ). The columns included in the output are lname, fname, state, and hphone. The output is titled \"Libname Sample 1: New Jersey Phone List\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         LNAME    FNAME STATE        HPHONE\n",
      "17      RHODES   JEREMY    NJ  201/812-1837\n",
      "62     ALVAREZ   CARLOS    NJ  201/732-8787\n",
      "66       DACKO    JASON    NJ  201/732-2323\n",
      "69   HENDERSON  WILLIAM    NJ  201/812-4789\n",
      "72     JOHNSON  JACKSON    NJ  201/732-3678\n",
      "73     MURPHEY     JOHN    NJ  201/812-4414\n",
      "74      PETERS  RANDALL    NJ  201/812-2478\n",
      "77     NEWKIRK  WILLIAM    NJ  201/732-6611\n",
      "79       ROUSE   JEREMY    NJ  201/732-9834\n",
      "81    FUJIHARA    KYOKO    NJ  201/812-0902\n",
      "85        VICK  THERESA    NJ  201/812-2424\n",
      "92      YANCEY    ROBIN    NJ  201/812-1874\n",
      "100   LAWRENCE    KATHY    NJ  201/812-3337\n",
      "101    NEWKIRK   SANDRA    NJ  201/812-3331\n",
      "109   BAREFOOT   JOSEPH    NJ  201/812-5665\n",
      "Number of rows: 15\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Sample 1: New Jersey Phone List\n",
    "\n",
    "# Assuming the DataFrame for SAMDAT7 has been read and is named df7\n",
    "import pandas as pd\n",
    "\n",
    "# Filter rows where the state is 'NJ' and select specific columns\n",
    "df1_nj = df7[df7['STATE'] == 'NJ'][['LNAME', 'FNAME', 'STATE', 'HPHONE']]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df1_nj)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df1_nj.shape[0]}')\n",
    "print(f'Number of columns: {df1_nj.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*=========================*/\n",
    " /* LIBNAME Sample 2       */\n",
    " /*=========================*/\n",
    "\n",
    "data work.highwage;\n",
    "  set samples.SAMDAT5(drop=sex birth hired);\n",
    "  if salary>60000 then\n",
    "    CATEGORY='High';\n",
    "  else if salary<30000 then\n",
    "    CATEGORY='Low';\n",
    "  else\n",
    "    CATEGORY='Avg';\n",
    "run;\n",
    "\n",
    "proc print data=work.highwage;\n",
    "  title 'Libname Sample 2: Salary Analysis';\n",
    "  format SALARY dollar10.2;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS program processes data from a dataset named SAMDAT5 within a library called samples. It drops the columns sex, birth, and hired, and creates a new variable CATEGORY based on the value of salary. The categories are defined as 'High' for salaries over 60,000, 'Low' for salaries under 30,000, and 'Avg' for anything in between. Finally, it prints the resulting dataset with the salary formatted as a dollar amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    IDNUM JOBCODE      SALARY CATEGORY\n",
      "0    1009     TA1  $28,880.00      Low\n",
      "1    1017     TA3  $40,858.00      Avg\n",
      "2    1036     TA3  $39,392.00      Avg\n",
      "3    1037     TA1  $28,558.00      Low\n",
      "4    1038     TA1  $26,533.00      Low\n",
      "..    ...     ...         ...      ...\n",
      "143  1970     FA1  $22,615.00      Low\n",
      "144  1983     FA3  $33,419.00      Avg\n",
      "145  1988     FA3  $32,217.00      Avg\n",
      "146  1991     TA1  $27,645.00      Low\n",
      "147  1995     ME1  $28,810.00      Low\n",
      "\n",
      "[148 rows x 4 columns]\n",
      "Number of rows: 148\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Sample 2: Salary Analysis\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df5 corresponds to SAMDAT5\n",
    "df2_highwage = df5.drop(columns=['SEX', 'BIRTH', 'HIRED'])\n",
    "\n",
    "# Categorize salary into 'High', 'Low', 'Avg'\n",
    "def categorize_salary(salary):\n",
    "    if salary > 60000:\n",
    "        return 'High'\n",
    "    elif salary < 30000:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Avg'\n",
    "\n",
    "df2_highwage['CATEGORY'] = df2_highwage['SALARY'].apply(categorize_salary)\n",
    "\n",
    "# Formatting the salary as dollar value\n",
    "df2_highwage['SALARY'] = df2_highwage['SALARY'].apply(lambda x: f'${x:,.2f}')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df2_highwage)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df2_highwage.shape[0]}')\n",
    "print(f'Number of columns: {df2_highwage.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*=========================*/\n",
    " /* LIBNAME Sample 3       */\n",
    " /*=========================*/\n",
    "\n",
    " data work.combined;\n",
    "  merge samples.SAMDAT7 samples.SAMDAT8(in=super\n",
    "    rename=(SUPID=IDNUM));\n",
    "  by IDNUM;\n",
    "  if super;\n",
    "run;\n",
    "\n",
    "proc print data=work.combined;\n",
    "  title 'Libname Sample 3: Supervisor Information';\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "The SAS program merges two datasets, SAMDAT7 and SAMDAT8 from the samples library. During the merge, it renames SUPID to IDNUM in SAMDAT8, uses IDNUM as the key for the merge, and includes only those records in the merged dataset that are present in SAMDAT8 (as indicated by the in=super condition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    IDNUM      LNAME      FNAME        CITY STATE        HPHONE\n",
      "0    1009     MORGAN     GEORGE    NEW YORK    NY  212/586-7753\n",
      "1    1017      WELCH     DARIUS    NEW YORK    NY  212/586-5535\n",
      "2    1036       WONG     LESLIE    NEW YORK    NY  212/587-2570\n",
      "3    1037       CHOW       JANE    STAMFORD    CT  203/781-8868\n",
      "4    1038  RODRIGUEZ      MARIA  BRIDGEPORT    CT  203/675-2048\n",
      "..    ...        ...        ...         ...   ...           ...\n",
      "143  1970     PARKER       ANNE    NEW YORK    NY  718/383-3895\n",
      "144  1983       DEAN     SHARON    NEW YORK    NY  718/384-1647\n",
      "145  1988     COOPER    ANTHONY    NEW YORK    NY  212/587-1228\n",
      "146  1991     HOWARD   GRETCHEN  BRIDGEPORT    CT  203/675-0007\n",
      "147  1995     VARNER  ELIZABETH    NEW YORK    NY  718/384-7113\n",
      "\n",
      "[148 rows x 6 columns]\n",
      "   SUPID STATE JOBCAT\n",
      "0   1106    CT     PT\n",
      "1   1118    NY     PT\n",
      "2   1126    NY     TA\n",
      "3   1352    NY     NA\n",
      "4   1385    CT     ME\n",
      "5   1401    NJ     TA\n",
      "6   1405    NJ     SC\n",
      "7   1417    NJ     NA\n",
      "8   1420    NJ     ME\n",
      "9   1431    CT     FA\n",
      "10  1433    NJ     FA\n",
      "11  1442    NJ     PT\n",
      "12  1564    NY     SC\n",
      "13  1639    CT     TA\n",
      "14  1677    CT     BC\n",
      "15  1834    NY     BC\n",
      "16  1882    NY     ME\n",
      "17  1935    CT     NA\n",
      "18  1983    NY     FA\n"
     ]
    }
   ],
   "source": [
    "print(df7)\n",
    "print(df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IDNUM         LNAME    FNAME        CITY STATE_x        HPHONE STATE_y  \\\n",
      "0   1106     MARSHBURN   JASPER    STAMFORD      CT  203/781-1457      CT   \n",
      "1   1118        DENNIS    ROGER    NEW YORK      NY  718/383-1122      NY   \n",
      "2   1126        KIMANI     ANNE    NEW YORK      NY  212/586-1229      NY   \n",
      "3   1352        RIVERS    SIMON    NEW YORK      NY  718/383-3345      NY   \n",
      "4   1385        RAYNOR   MILTON  BRIDGEPORT      CT  203/675-2846      CT   \n",
      "5   1401       ALVAREZ   CARLOS    PATERSON      NJ  201/732-8787      NJ   \n",
      "6   1405         DACKO    JASON    PATERSON      NJ  201/732-2323      NJ   \n",
      "7   1417       NEWKIRK  WILLIAM    PATERSON      NJ  201/732-6611      NJ   \n",
      "8   1420         ROUSE   JEREMY    PATERSON      NJ  201/732-9834      NJ   \n",
      "9   1431         YOUNG  DEBORAH    STAMFORD      CT  203/781-2987      CT   \n",
      "10  1433        YANCEY    ROBIN   PRINCETON      NJ  201/812-1874      NJ   \n",
      "11  1442       NEWKIRK   SANDRA   PRINCETON      NJ  201/812-3331      NJ   \n",
      "12  1564       WALTERS     ANNE    NEW YORK      NY  212/587-3257      NY   \n",
      "13  1639  CARTER-COHEN    KAREN    STAMFORD      CT  203/781-8839      CT   \n",
      "14  1677        KRAMER  JACKSON  BRIDGEPORT      CT  203/675-7432      CT   \n",
      "15  1834       LEBLANC  RUSSELL    NEW YORK      NY  718/384-0040      NY   \n",
      "16  1882        TUCKER     ALAN    NEW YORK      NY  718/384-0216      NY   \n",
      "17  1935     FERNANDEZ  KATRINA  BRIDGEPORT      CT  203/675-2962      CT   \n",
      "18  1983          DEAN   SHARON    NEW YORK      NY  718/384-1647      NY   \n",
      "\n",
      "   JOBCAT  \n",
      "0      PT  \n",
      "1      PT  \n",
      "2      TA  \n",
      "3      NA  \n",
      "4      ME  \n",
      "5      TA  \n",
      "6      SC  \n",
      "7      NA  \n",
      "8      ME  \n",
      "9      FA  \n",
      "10     FA  \n",
      "11     PT  \n",
      "12     SC  \n",
      "13     TA  \n",
      "14     BC  \n",
      "15     BC  \n",
      "16     ME  \n",
      "17     NA  \n",
      "18     FA  \n",
      "Number of rows: 19\n",
      "Number of columns: 8\n"
     ]
    }
   ],
   "source": [
    "# Sample 3: Supervisor Information\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrames df7 and df8 correspond to SAMDAT7 and SAMDAT8\n",
    "# Renaming SUPID to IDNUM in df8 before merging\n",
    "df8_renamed = df8.rename(columns={'SUPID': 'IDNUM'})\n",
    "\n",
    "# Merging df7 and df8 on IDNUM\n",
    "# Using an indicator to replicate the 'in=super' SAS functionality\n",
    "df3_combined = pd.merge(df7, df8_renamed, on='IDNUM', how='inner', indicator=True)\n",
    "\n",
    "# Filter rows to include only those that come from df8 (as per 'if super;' in SAS)\n",
    "df3_combined = df3_combined[df3_combined['_merge'] == 'both']\n",
    "\n",
    "# Drop the merge indicator column used for filtering\n",
    "df3_combined.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df3_combined)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df3_combined.shape[0]}')\n",
    "print(f'Number of columns: {df3_combined.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 4       */\n",
    " /*=========================*/\n",
    "\n",
    "data work.payroll;\n",
    "  update samples.SAMDAT5\n",
    "         samples.SAMDAT6;\n",
    "  by IDNUM;\n",
    "run;\n",
    "\n",
    "proc print data=work.payroll;\n",
    "  title 'Libname Sample 4: Updated Payroll Data';\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS program uses the update statement to combine two datasets, SAMDAT5 and SAMDAT6 from the samples library, based on the IDNUM column. The update operation in SAS replaces or updates rows in the master dataset (SAMDAT5) with rows from the transaction dataset (SAMDAT6) when the IDNUM values match. If IDNUM is not present in SAMDAT5, the row from SAMDAT6 is not addedâ€”this differs from a full merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    IDNUM SEX JOBCODE   SALARY      BIRTH      HIRED\n",
      "0    1009   M     TA1  28880.0 1959-03-02 1992-03-26\n",
      "1    1017   M     TA3  40858.0 1957-12-28 1981-10-16\n",
      "2    1036   F     TA3  39392.0 1965-05-19 1984-10-23\n",
      "3    1037   F     TA1  28558.0 1964-04-10 1992-09-13\n",
      "4    1038   F     TA1  26533.0 1969-11-09 1991-11-23\n",
      "..    ...  ..     ...      ...        ...        ...\n",
      "145  1983   F     FA3  33419.0 1962-02-28 1987-04-27\n",
      "146  1988   M     FA3  32217.0 1959-11-30 1984-09-18\n",
      "147  1991   F     TA1  27645.0 1972-05-07 1992-12-12\n",
      "148  1995   F     ME1  28810.0 1973-08-24 1993-09-19\n",
      "149  1998   M     SCP  23100.0 1970-09-10 1992-11-02\n",
      "\n",
      "[150 rows x 6 columns]\n",
      "Number of rows: 150\n",
      "Number of columns: 6\n"
     ]
    }
   ],
   "source": [
    "# Sample 4: Updated Payroll Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrames df5 and df6 correspond to SAMDAT5 and SAMDAT6\n",
    "# Setting IDNUM as the index for proper updating\n",
    "df5.set_index('IDNUM', inplace=True)\n",
    "df6.set_index('IDNUM', inplace=True)\n",
    "\n",
    "# Update df5 with df6\n",
    "df4_payroll = df5.combine_first(df6)\n",
    "\n",
    "# Reset index to move IDNUM back to a column\n",
    "df4_payroll.reset_index(inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df4_payroll)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df4_payroll.shape[0]}')\n",
    "print(f'Number of columns: {df4_payroll.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 5       */\n",
    " /*=========================*/\n",
    "title 'Libname Sample 5: Total Salary by Jobcode';\n",
    "\n",
    "proc sql;\n",
    "  select JOBCODE label='Jobcode',\n",
    "         sum(SALARY) as total\n",
    "         label='Total for Group'\n",
    "         format=dollar11.2\n",
    "  from samples.SAMDAT5\n",
    "  group by JOBCODE;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS program calculates the total salary by job code from the dataset SAMDAT5. It groups by JOBCODE, and the total salary for each job code is formatted as a dollar amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Jobcode Total for Group\n",
      "0      BCK     $232,148.00\n",
      "1      FA1     $253,433.00\n",
      "2      FA2     $447,790.00\n",
      "3      FA3     $230,537.00\n",
      "4      ME1     $228,002.00\n",
      "5      ME2     $498,076.00\n",
      "6      ME3     $296,875.00\n",
      "7      NA1     $210,161.00\n",
      "8      NA2     $157,149.00\n",
      "9      PT1     $543,264.00\n",
      "10     PT2     $879,252.00\n",
      "11     PT3     $221,009.00\n",
      "12     SCP     $128,162.00\n",
      "13     TA1     $249,492.00\n",
      "14     TA2     $671,499.00\n",
      "15     TA3     $476,155.00\n",
      "Number of rows: 16\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Sample 5: Total Salary by Jobcode\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df5 corresponds to SAMDAT5\n",
    "# Calculating the sum of SALARY grouped by JOBCODE\n",
    "df5_salary_group = df5.groupby('JOBCODE')['SALARY'].sum().reset_index()\n",
    "\n",
    "# Renaming the columns to match SAS output\n",
    "df5_salary_group.columns = ['Jobcode', 'Total for Group']\n",
    "\n",
    "# Formatting the total salary as dollar value\n",
    "df5_salary_group['Total for Group'] = df5_salary_group['Total for Group'].apply(lambda x: f'${x:,.2f}')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df5_salary_group)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df5_salary_group.shape[0]}')\n",
    "print(f'Number of columns: {df5_salary_group.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 6       */\n",
    " /*=========================*/\n",
    "\n",
    "title 'Libname Sample 6: Flights to London and Frankfurt';\n",
    "\n",
    "proc sql;\n",
    "  select DATES, DEST from samples.SAMDAT2\n",
    "  where (DEST eq \"FRA\") or\n",
    "    (DEST eq \"LON\")\n",
    "  order by DEST;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS program selects records from the SAMDAT2 dataset, filtering to include only flights headed to London (\"LON\") or Frankfurt (\"FRA\"). The results are ordered by the destination (DEST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DATES DEST\n",
      "3  1998-03-01  FRA\n",
      "10 1998-03-02  FRA\n",
      "17 1998-03-03  FRA\n",
      "24 1998-03-04  FRA\n",
      "31 1998-03-05  FRA\n",
      "42 1998-03-07  FRA\n",
      "2  1998-03-01  LON\n",
      "9  1998-03-02  LON\n",
      "16 1998-03-03  LON\n",
      "23 1998-03-04  LON\n",
      "30 1998-03-05  LON\n",
      "36 1998-03-06  LON\n",
      "41 1998-03-07  LON\n",
      "Number of rows: 13\n",
      "Number of columns: 7\n"
     ]
    }
   ],
   "source": [
    "# Sample 6: Flights to London and Frankfurt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df2 corresponds to SAMDAT2\n",
    "# Filtering for flights to Frankfurt (FRA) and London (LON)\n",
    "df6_flights = df2[df2['DEST'].isin(['FRA', 'LON'])]\n",
    "\n",
    "# Sorting by destination\n",
    "df6_flights = df6_flights.sort_values(by='DEST')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df6_flights[['DATES', 'DEST']])\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df6_flights.shape[0]}')\n",
    "print(f'Number of columns: {df6_flights.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 7       */\n",
    " /*=========================*/\n",
    "\n",
    "proc sql;\n",
    "   title  'Libname Sample 7: International Flights by Flight Number';\n",
    "   title2 'with Over 200 Passengers';\n",
    "   select FLIGHT   label=\"Flight Number\",\n",
    "          DATES    label=\"Departure Date\",\n",
    "          DEST     label=\"Destination\",\n",
    "          BOARDED  label=\"Number Boarded\"\n",
    "     from samples.SAMDAT3\n",
    "    where BOARDED > 200\n",
    "    order by FLIGHT;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This program selects flight information from the SAMDAT3 dataset for flights that boarded more than 200 passengers. It retrieves flight number, departure date, destination, and number boarded, ordering the results by flight number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Flight Number Departure Date Destination  Number Boarded\n",
      "12           219     1998-03-04         LON           232.0\n",
      "22           219     1998-03-07         LON           241.0\n",
      "1            622     1998-03-01         FRA           207.0\n",
      "23           622     1998-03-07         FRA           210.0\n",
      "Number of rows: 4\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Sample 7: International Flights by Flight Number with Over 200 Passengers\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df3 corresponds to SAMDAT3\n",
    "# Filtering for flights with more than 200 passengers boarded\n",
    "df7_flights = df3[df3['BOARDED'] > 200]\n",
    "\n",
    "# Sorting by flight number\n",
    "df7_flights = df7_flights.sort_values(by='FLIGHT')\n",
    "\n",
    "# Display the DataFrame with specified columns and custom labels\n",
    "print(df7_flights[['FLIGHT', 'DATES', 'DEST', 'BOARDED']].rename(columns={\n",
    "    'FLIGHT': 'Flight Number',\n",
    "    'DATES': 'Departure Date',\n",
    "    'DEST': 'Destination',\n",
    "    'BOARDED': 'Number Boarded'\n",
    "}))\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df7_flights.shape[0]}')\n",
    "print(f'Number of columns: {df7_flights.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*=========================*/\n",
    " /* LIBNAME Sample 8        */\n",
    " /*=========================*/\n",
    "\n",
    "title 'Libname Sample 8: Employees with salary greater than $40,000';\n",
    "\n",
    "proc sql;\n",
    "  select a.LNAME, a.FNAME, b.SALARY\n",
    "    format=dollar10.2\n",
    "  from samples.SAMDAT7 a, samples.SAMDAT5 b\n",
    "  where (a.IDNUM eq b.IDNUM) and\n",
    "    (b.SALARY gt 40000);\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS program performs a join between two datasets, SAMDAT7 and SAMDAT5, matching by IDNUM. It then selects employees with salaries greater than $40,000, displaying their last name, first name, and salary formatted as a dollar amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            LNAME      FNAME       SALARY\n",
      "1           WELCH     DARIUS   $40,858.00\n",
      "7          VENTER    RANDALL   $66,558.00\n",
      "15      MARSHBURN     JASPER   $89,632.00\n",
      "16       THOMPSON      WAYNE   $89,977.00\n",
      "17         RHODES     JEREMY   $40,586.00\n",
      "24         DENNIS      ROGER  $111,379.00\n",
      "32         KIMANI       ANNE   $40,899.00\n",
      "45         CASTON   FRANKLIN   $41,690.00\n",
      "47     STEPHENSON       ADAM   $42,178.00\n",
      "48       BANADYGA     JUSTIN   $88,606.00\n",
      "49         O'NEAL      BRYAN   $40,079.00\n",
      "51         RIVERS      SIMON   $53,798.00\n",
      "56         MORGAN     ALFRED   $42,264.00\n",
      "58         RAYNOR     MILTON   $43,900.00\n",
      "65          COHEN        LEE   $91,376.00\n",
      "68      GREGORSKI     DANIEL   $68,096.00\n",
      "70        HAVELKA    RAYMOND   $41,551.00\n",
      "71         HARRIS    CHARLES   $84,685.00\n",
      "77        NEWKIRK    WILLIAM   $52,270.00\n",
      "79          ROUSE     JEREMY   $43,071.00\n",
      "87          BRADY  CHRISTINE   $68,767.00\n",
      "98     HASENHAUER  CHRISTINA   $70,736.00\n",
      "101       NEWKIRK     SANDRA   $84,536.00\n",
      "102         WELLS      AGNES   $42,274.00\n",
      "106        NEWTON      JAMES   $84,203.00\n",
      "109      BAREFOOT     JOSEPH   $43,025.00\n",
      "110        PARKER        JAY   $41,526.00\n",
      "111       HERRERO      CLYDE   $66,130.00\n",
      "113    PENNINGTON    MICHAEL   $71,349.00\n",
      "118  CARTER-COHEN      KAREN   $40,260.00\n",
      "125    BRANCACCIO     JOSEPH   $66,517.00\n",
      "126        LUFKIN        ROY  $109,630.00\n",
      "129         TRIPP      KATHY   $84,471.00\n",
      "131        NORRIS      DIANE   $43,433.00\n",
      "134        TUCKER       ALAN   $41,538.00\n",
      "135    STEPHENSON     ROBERT   $91,908.00\n",
      "137        GRAHAM      ALVIN   $65,111.00\n",
      "141      UPCHURCH      LARRY   $89,858.00\n",
      "142     FERNANDEZ    KATRINA   $51,081.00\n",
      "Number of rows: 39\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "# Sample 8: Employees with salary greater than $40,000\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrames df7 and df5 correspond to SAMDAT7 and SAMDAT5\n",
    "# Merging data on IDNUM\n",
    "df8_employees = pd.merge(df7, df5, on='IDNUM')\n",
    "\n",
    "# Filtering for salaries greater than 40,000\n",
    "df8_employees = df8_employees[df8_employees['SALARY'] > 40000]\n",
    "\n",
    "# Formatting the salary as dollar value\n",
    "df8_employees['SALARY'] = df8_employees['SALARY'].apply(lambda x: f'${x:,.2f}')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df8_employees[['LNAME', 'FNAME', 'SALARY']])\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df8_employees.shape[0]}')\n",
    "print(f'Number of columns: {df8_employees.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 9         */\n",
    " /*==========================*/\n",
    "\n",
    "/* SQL Implicit Passthru ON */\n",
    "title 'Libname Sample 9a: Delayed International Flights in March';\n",
    "\n",
    "proc sql;\n",
    "  select distinct samdat1.FLIGHT,\n",
    "      samdat1.DATES,\n",
    "      DELAY format=2.0\n",
    "    from samples.SAMDAT1, samples.SAMDAT2, samples.SAMDAT3\n",
    "  where samdat1.FLIGHT=samdat2.FLIGHT and\n",
    "        samdat1.DATES=samdat2.DATES and\n",
    "        samdat1.FLIGHT=samdat3.FLIGHT and\n",
    "        DELAY>0\n",
    "  order by DELAY descending;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS program performs a multi-table join on three datasets (SAMDAT1, SAMDAT2, SAMDAT3) based on flight number and dates. It selects flights with a delay greater than zero and orders the results by the delay in descending order. The query focuses on distinct records, indicating a need to avoid duplicate entries in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   FLIGHT    46 non-null     object        \n",
      " 1   DATES     46 non-null     datetime64[ns]\n",
      " 2   DEPART    46 non-null     float64       \n",
      " 3   ORIG      46 non-null     object        \n",
      " 4   DEST      46 non-null     object        \n",
      " 5   MILES     46 non-null     float64       \n",
      " 6   BOARDED   46 non-null     float64       \n",
      " 7   CAPACITY  46 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(4), object(3)\n",
      "memory usage: 3.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   FLIGHT    46 non-null     object        \n",
      " 1   DATES     46 non-null     datetime64[ns]\n",
      " 2   ORIG      46 non-null     object        \n",
      " 3   DEST      46 non-null     object        \n",
      " 4   DELAYCAT  46 non-null     object        \n",
      " 5   DESTYPE   46 non-null     object        \n",
      " 6   DELAY     46 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(5)\n",
      "memory usage: 2.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26 entries, 0 to 25\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   FLIGHT   26 non-null     object        \n",
      " 1   DATES    26 non-null     datetime64[ns]\n",
      " 2   DEST     26 non-null     object        \n",
      " 3   BOARDED  26 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(2)\n",
      "memory usage: 964.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df1.info()\n",
    "df2.info()\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Index(['DATES'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30200\\2393069160.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Filtering for delays greater than 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdf9_flights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf9_flights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf9_flights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DELAY'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Removing duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf9_flights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf9_flights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FLIGHT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DATES'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DELAY'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Sorting by delay in descending order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdf9_flights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf9_flights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'DELAY'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6802\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6803\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ignore_index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6805\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6806\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6807\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6933\u001b[0m         \u001b[1;31m# Otherwise, raise a KeyError, same as if you try to __getitem__ with a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6934\u001b[0m         \u001b[1;31m# key that doesn't exist.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6935\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6936\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6937\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6939\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6940\u001b[0m             \u001b[1;31m# GH#45236 This is faster than get_group_index below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: Index(['DATES'], dtype='object')"
     ]
    }
   ],
   "source": [
    "# Sample 9a: Delayed International Flights in March\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrames df1, df2, and df3 correspond to SAMDAT1, SAMDAT2, and SAMDAT3\n",
    "# Performing the joins\n",
    "df9_flights = pd.merge(df1, df2, on=['FLIGHT', 'DATES'])\n",
    "df9_flights = pd.merge(df9_flights, df3, on=['FLIGHT'])\n",
    "\n",
    "# Filtering for delays greater than 0\n",
    "df9_flights = df9_flights[df9_flights['DELAY'] > 0]\n",
    "\n",
    "# Removing duplicates\n",
    "df9_flights = df9_flights.drop_duplicates(subset=['FLIGHT', 'DATES', 'DELAY'])\n",
    "\n",
    "# Sorting by delay in descending order\n",
    "df9_flights = df9_flights.sort_values(by='DELAY', ascending=False)\n",
    "\n",
    "# Formatting the delay to show only two decimal places\n",
    "df9_flights['DELAY'] = df9_flights['DELAY'].map('{:.2f}'.format)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df9_flights[['FLIGHT', 'DATES', 'DELAY']])\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df9_flights.shape[0]}')\n",
    "print(f'Number of columns: {df9_flights.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FLIGHT      DATES  DELAY\n",
      "23    622 1998-03-04  30.00\n",
      "12    219 1998-03-06  27.00\n",
      "25    622 1998-03-07  21.00\n",
      "8     219 1998-03-02  18.00\n",
      "7     219 1998-03-01  18.00\n",
      "13    219 1998-03-07  15.00\n",
      "0     132 1998-03-01  14.00\n",
      "5     132 1998-03-06   7.00\n",
      "2     132 1998-03-03   6.00\n",
      "1     132 1998-03-02   5.00\n",
      "14    271 1998-03-01   5.00\n",
      "17    271 1998-03-04   5.00\n",
      "18    271 1998-03-05   5.00\n",
      "9     219 1998-03-03   4.00\n",
      "15    271 1998-03-02   4.00\n",
      "19    271 1998-03-07   4.00\n",
      "10    219 1998-03-04   3.00\n",
      "11    219 1998-03-05   3.00\n",
      "4     132 1998-03-05   3.00\n",
      "16    271 1998-03-03   2.00\n",
      "Number of rows: 20\n",
      "Number of columns: 15\n"
     ]
    }
   ],
   "source": [
    "# Sample 9a: Delayed International Flights in March\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrames df1, df2, and df3 correspond to SAMDAT1, SAMDAT2, and SAMDAT3\n",
    "# Ensure all DataFrame column names are in uppercase to avoid case sensitivity issues\n",
    "df1.columns = [x.upper() for x in df1.columns]\n",
    "df2.columns = [x.upper() for x in df2.columns]\n",
    "df3.columns = [x.upper() for x in df3.columns]\n",
    "\n",
    "# Check if 'FLIGHT' and 'DATES' columns are available in df1 and df2\n",
    "if not all(col in df1.columns and col in df2.columns for col in ['FLIGHT', 'DATES']):\n",
    "    print(\"Error: Necessary columns ['FLIGHT', 'DATES'] are missing in df1 or df2.\")\n",
    "else:\n",
    "    df9_flights = pd.merge(df1, df2, on=['FLIGHT', 'DATES'], how='inner')\n",
    "\n",
    "    # Check if 'FLIGHT' is present in df3 for the merge\n",
    "    if 'FLIGHT' in df3.columns and 'DATES' in df3.columns:\n",
    "        df9_flights = pd.merge(df9_flights, df3, on=['FLIGHT', 'DATES'], how='inner')\n",
    "    elif 'FLIGHT' in df3.columns:\n",
    "        df9_flights = pd.merge(df9_flights, df3, on='FLIGHT', how='inner')\n",
    "    else:\n",
    "        print(\"Error: 'FLIGHT' column is missing in df3.\")\n",
    "\n",
    "    # Check if 'DELAY' is in the DataFrame after merging\n",
    "    if 'DELAY' in df9_flights.columns:\n",
    "        # Filtering for delays greater than 0\n",
    "        df9_flights = df9_flights[df9_flights['DELAY'] > 0]\n",
    "        df9_flights = df9_flights.drop_duplicates(subset=['FLIGHT', 'DATES', 'DELAY'])\n",
    "\n",
    "        # Sorting by delay in descending order\n",
    "        df9_flights = df9_flights.sort_values(by='DELAY', ascending=False)\n",
    "\n",
    "        # Formatting the delay to show only two decimal places\n",
    "        df9_flights['DELAY'] = df9_flights['DELAY'].map('{:.2f}'.format)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(df9_flights[['FLIGHT', 'DATES', 'DELAY']])\n",
    "    else:\n",
    "        print(\"Error: 'DELAY' column is missing after merging.\")\n",
    "\n",
    "    # Show the number of columns and rows for the data output\n",
    "    print(f'Number of rows: {df9_flights.shape[0]}')\n",
    "    print(f'Number of columns: {df9_flights.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 9c        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 9c: Delayed International Flights in March';\n",
    "\n",
    "proc sql;\n",
    "  select distinct samdat1.FLIGHT,\n",
    "      samdat1.DATES,\n",
    "      DELAY format=2.0\n",
    "    from samples.SAMDAT1\n",
    "    full join samples.SAMDAT2 on\n",
    "      samdat1.FLIGHT = samdat2.FLIGHT\n",
    "    full join samples.SAMDAT3 on\n",
    "      samdat1.FLIGHT = samdat3.FLIGHT\n",
    "  order by DELAY descending;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS program joins three datasets (SAMDAT1, SAMDAT2, SAMDAT3) using a full join based on flight number. It selects distinct records for international flights showing the flight number, date, and delay, ordering the results by the delay in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     FLIGHT      DATES  DELAY\n",
      "1054    622 1998-03-01  30.00\n",
      "1055    622 1998-03-02  30.00\n",
      "1056    622 1998-03-03  30.00\n",
      "1057    622 1998-03-04  30.00\n",
      "1058    622 1998-03-05  30.00\n",
      "...     ...        ...    ...\n",
      "1061    622 1998-03-02  -6.00\n",
      "1064    622 1998-03-05  -6.00\n",
      "1065    622 1998-03-07  -6.00\n",
      "1060    622 1998-03-01  -6.00\n",
      "1063    622 1998-03-04  -6.00\n",
      "\n",
      "[154 rows x 3 columns]\n",
      "Number of rows: 154\n",
      "Number of columns: 17\n"
     ]
    }
   ],
   "source": [
    "# Sample 9c: Delayed International Flights in March\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrames df1, df2, and df3 correspond to SAMDAT1, SAMDAT2, and SAMDAT3\n",
    "# Performing the full joins\n",
    "df9c_flights = pd.merge(df1, df2, on='FLIGHT', how='outer')\n",
    "df9c_flights = pd.merge(df9c_flights, df3, on='FLIGHT', how='outer')\n",
    "\n",
    "# Removing duplicates\n",
    "df9c_flights = df9c_flights.drop_duplicates(subset=['FLIGHT', 'DATES', 'DELAY'])\n",
    "\n",
    "# Sorting by delay in descending order\n",
    "df9c_flights = df9c_flights.sort_values(by='DELAY', ascending=False)\n",
    "\n",
    "# Formatting the delay to show only two decimal places\n",
    "df9c_flights['DELAY'] = df9c_flights['DELAY'].map('{:.2f}'.format)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df9c_flights[['FLIGHT', 'DATES', 'DELAY']])\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df9c_flights.shape[0]}')\n",
    "print(f'Number of columns: {df9c_flights.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*==========================*/\n",
    " /* LIBNAME Sample 10        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 10: Payrolls 1 & 2';\n",
    "\n",
    "proc sql;\n",
    "  select IDNUM, SEX, JOBCODE, SALARY,\n",
    "         BIRTH,\n",
    "         HIRED\n",
    "     from samples.SAMDAT5\n",
    "  outer union corr\n",
    "  select *\n",
    "     from samples.SAMDAT6\n",
    "   order by IDNUM, JOBCODE, SALARY;\n",
    "quit;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary:\n",
    "This SAS program performs an outer union of two datasets, SAMDAT5 and SAMDAT6, effectively merging their records while keeping all columns aligned by name. It ensures that records unique to each dataset are included and then sorts the results by IDNUM, JOBCODE, and SALARY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['IDNUM'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming DataFrame df5 corresponds to SAMDAT5 and DataFrame df6 to SAMDAT6\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Making copies of the dataframes with only the required columns for union operation\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df10_df5_subset \u001b[38;5;241m=\u001b[39m \u001b[43mdf5\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIDNUM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSEX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJOBCODE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSALARY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBIRTH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHIRED\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      8\u001b[0m df10_df6_subset \u001b[38;5;241m=\u001b[39m df6[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIDNUM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOBCODE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSALARY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIRTH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHIRED\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Concatenating subsets and dropping duplicates to mimic 'outer union corr'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['IDNUM'] not in index\""
     ]
    }
   ],
   "source": [
    "# Libname Sample 10: Payrolls 1 & 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df5 corresponds to SAMDAT5 and DataFrame df6 to SAMDAT6\n",
    "# Making copies of the dataframes with only the required columns for union operation\n",
    "df10_df5_subset = df5[['IDNUM', 'SEX', 'JOBCODE', 'SALARY', 'BIRTH', 'HIRED']].copy()\n",
    "df10_df6_subset = df6[['IDNUM', 'SEX', 'JOBCODE', 'SALARY', 'BIRTH', 'HIRED']].copy()\n",
    "\n",
    "# Concatenating subsets and dropping duplicates to mimic 'outer union corr'\n",
    "df10_payrolls = pd.concat([df10_df5_subset, df10_df6_subset]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Sorting the DataFrame\n",
    "df10_payrolls = df10_payrolls.sort_values(by=['IDNUM', 'JOBCODE', 'SALARY'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df10_payrolls)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df10_payrolls.shape[0]}')\n",
    "print(f'Number of columns: {df10_payrolls.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    IDNUM SEX JOBCODE   SALARY      BIRTH      HIRED\n",
      "0    1009   M     TA1  28880.0 1959-03-02 1992-03-26\n",
      "1    1017   M     TA3  40858.0 1957-12-28 1981-10-16\n",
      "2    1036   F     TA3  39392.0 1965-05-19 1984-10-23\n",
      "148  1036   F     TA3  42465.0 1965-05-19 1984-10-23\n",
      "3    1037   F     TA1  28558.0 1964-04-10 1992-09-13\n",
      "..    ...  ..     ...      ...        ...        ...\n",
      "144  1983   F     FA3  33419.0 1962-02-28 1987-04-27\n",
      "145  1988   M     FA3  32217.0 1959-11-30 1984-09-18\n",
      "146  1991   F     TA1  27645.0 1972-05-07 1992-12-12\n",
      "147  1995   F     ME1  28810.0 1973-08-24 1993-09-19\n",
      "159  1998   M     SCP  23100.0 1970-09-10 1992-11-02\n",
      "\n",
      "[160 rows x 6 columns]\n",
      "Number of rows: 160\n",
      "Number of columns: 6\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 10: Payrolls 1 & 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df5 corresponds to SAMDAT5 and DataFrame df6 to SAMDAT6\n",
    "# Ensure column names are uppercase to avoid case sensitivity issues\n",
    "df5.columns = [x.upper() for x in df5.columns]\n",
    "df6.columns = [x.upper() for x in df6.columns]\n",
    "\n",
    "# Check if required columns are present in both dataframes\n",
    "required_columns = ['IDNUM', 'SEX', 'JOBCODE', 'SALARY', 'BIRTH', 'HIRED']\n",
    "missing_df5 = [col for col in required_columns if col not in df5.columns]\n",
    "missing_df6 = [col for col in required_columns if col not in df6.columns]\n",
    "\n",
    "if missing_df5 or missing_df6:\n",
    "    print(f\"Missing columns in df5: {missing_df5}\")\n",
    "    print(f\"Missing columns in df6: {missing_df6}\")\n",
    "else:\n",
    "    # Making copies of the dataframes with only the required columns for union operation\n",
    "    df10_df5_subset = df5[required_columns].copy()\n",
    "    df10_df6_subset = df6[required_columns].copy()\n",
    "\n",
    "    # Concatenating subsets and dropping duplicates to mimic 'outer union corr'\n",
    "    df10_payrolls = pd.concat([df10_df5_subset, df10_df6_subset]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Sorting the DataFrame\n",
    "    df10_payrolls = df10_payrolls.sort_values(by=['IDNUM', 'JOBCODE', 'SALARY'])\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df10_payrolls)\n",
    "\n",
    "    # Show the number of columns and rows for the data output\n",
    "    print(f'Number of rows: {df10_payrolls.shape[0]}')\n",
    "    print(f'Number of columns: {df10_payrolls.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 11        */\n",
    " /*==========================*/\n",
    "\n",
    "proc sql undo_policy=none;\n",
    "insert into samples.SAMDAT8\n",
    "\tvalues('1588','NY','FA');\n",
    "quit;\n",
    "\n",
    "proc print data=samples.SAMDAT8;\n",
    "\ttitle 'Libname Sample 11: New Row in AIRLINE.SAMDAT8';\n",
    "run;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "The SAS code performs an insert operation to add a new row to the dataset SAMDAT8. The new row contains specific values for given columns, presumably identified as ID, state, and job code or function. After the insertion, the dataset is printed with a title indicating the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1840\\2578685893.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Assuming DataFrame df8 corresponds to SAMDAT8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Creating a new row and appending it to df8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnew_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'COLUMN1'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'1588'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'COLUMN2'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'NY'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'COLUMN3'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'FA'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf11_samdat8\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Display the updated DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf11_samdat8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6293\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6294\u001b[0m         ):\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6296\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# Libname Sample 11: New Row in AIRLINE.SAMDAT8\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df8 corresponds to SAMDAT8\n",
    "# Creating a new row and appending it to df8\n",
    "new_row = pd.DataFrame({'COLUMN1': ['1588'], 'COLUMN2': ['NY'], 'COLUMN3': ['FA']})\n",
    "df11_samdat8 = df8.append(new_row, ignore_index=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df11_samdat8)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df11_samdat8.shape[0]}')\n",
    "print(f'Number of columns: {df11_samdat8.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 13        */\n",
    " /*==========================*/\n",
    "\n",
    "proc sql;\n",
    "\n",
    "  create table work.gtforty as\n",
    "  select LNAME as lastname,\n",
    "         FNAME as firstname,\n",
    "         SALARY as Salary\n",
    "  from samples.SAMDAT7 a, samples.SAMDAT5 b\n",
    "  where (a.IDNUM eq b.IDNUM) and (SALARY gt 40000);\n",
    "\n",
    "quit;\n",
    "\n",
    "proc print data=work.gtforty noobs;\n",
    "  title 'Libname Sample 13: Employees with salaries over $40,000';\n",
    "  format SALARY dollar10.2;\n",
    "\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS code joins two datasets, SAMDAT7 and SAMDAT5, on a common field (IDNUM) and selects employees with salaries greater than $40,000. It creates a new table work.gtforty containing last name, first name, and salary of these employees. The result is printed with salary values formatted as dollar amounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['IDNUM'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming df7 corresponds to SAMDAT7 and df5 to SAMDAT5\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Merging dataframes on IDNUM\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df13_merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df7[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIDNUM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLNAME\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFNAME\u001b[39m\u001b[38;5;124m'\u001b[39m]], \u001b[43mdf5\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIDNUM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSALARY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIDNUM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Filtering for salaries greater than $40,000\u001b[39;00m\n\u001b[0;32m     10\u001b[0m df13_gtforty \u001b[38;5;241m=\u001b[39m df13_merged[df13_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSALARY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m40000\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['IDNUM'] not in index\""
     ]
    }
   ],
   "source": [
    "# Libname Sample 13: Employees with salaries over $40,000\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df7 corresponds to SAMDAT7 and df5 to SAMDAT5\n",
    "# Merging dataframes on IDNUM\n",
    "df13_merged = pd.merge(df7[['IDNUM', 'LNAME', 'FNAME']], df5[['IDNUM', 'SALARY']], on='IDNUM')\n",
    "\n",
    "# Filtering for salaries greater than $40,000\n",
    "df13_gtforty = df13_merged[df13_merged['SALARY'] > 40000]\n",
    "\n",
    "# Renaming columns as specified in the SAS code\n",
    "df13_gtforty = df13_gtforty.rename(columns={'LNAME': 'LASTNAME', 'FNAME': 'FIRSTNAME', 'SALARY': 'Salary'})\n",
    "\n",
    "# Formatting the salary as dollar values\n",
    "df13_gtforty['Salary'] = df13_gtforty['Salary'].apply(lambda x: f'${x:,.2f}')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df13_gtforty)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df13_gtforty.shape[0]}')\n",
    "print(f'Number of columns: {df13_gtforty.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SUPID STATE JOBCAT COLUMN1 COLUMN2 COLUMN3\n",
      "0   1106    CT     PT     NaN     NaN     NaN\n",
      "1   1118    NY     PT     NaN     NaN     NaN\n",
      "2   1126    NY     TA     NaN     NaN     NaN\n",
      "3   1352    NY     NA     NaN     NaN     NaN\n",
      "4   1385    CT     ME     NaN     NaN     NaN\n",
      "5   1401    NJ     TA     NaN     NaN     NaN\n",
      "6   1405    NJ     SC     NaN     NaN     NaN\n",
      "7   1417    NJ     NA     NaN     NaN     NaN\n",
      "8   1420    NJ     ME     NaN     NaN     NaN\n",
      "9   1431    CT     FA     NaN     NaN     NaN\n",
      "10  1433    NJ     FA     NaN     NaN     NaN\n",
      "11  1442    NJ     PT     NaN     NaN     NaN\n",
      "12  1564    NY     SC     NaN     NaN     NaN\n",
      "13  1639    CT     TA     NaN     NaN     NaN\n",
      "14  1677    CT     BC     NaN     NaN     NaN\n",
      "15  1834    NY     BC     NaN     NaN     NaN\n",
      "16  1882    NY     ME     NaN     NaN     NaN\n",
      "17  1935    CT     NA     NaN     NaN     NaN\n",
      "18  1983    NY     FA     NaN     NaN     NaN\n",
      "19   NaN   NaN    NaN    1588      NY      FA\n",
      "Number of rows: 20\n",
      "Number of columns: 6\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 11: New Row in AIRLINE.SAMDAT8\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df8 corresponds to SAMDAT8\n",
    "# Creating a new row and appending it to df8 using pd.concat()\n",
    "new_row = pd.DataFrame({'COLUMN1': ['1588'], 'COLUMN2': ['NY'], 'COLUMN3': ['FA']})\n",
    "df11_samdat8 = pd.concat([df8, new_row], ignore_index=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df11_samdat8)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df11_samdat8.shape[0]}')\n",
    "print(f'Number of columns: {df11_samdat8.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SUPID STATE JOBCAT\n",
      "0   1106    CT     PT\n",
      "1   1118    NY     PT\n",
      "2   1126    NY     TA\n",
      "3   1352    NY     NA\n",
      "4   1385    CT     ME\n",
      "5   1401    NJ     TA\n",
      "6   1405    NJ     SC\n",
      "7   1417    NJ     NA\n",
      "8   1420    NJ     ME\n",
      "9   1431    CT     FA\n",
      "10  1433    NJ     FA\n",
      "11  1442    NJ     PT\n",
      "12  1564    NY     SC\n",
      "13  1639    CT     TA\n",
      "14  1677    CT     BC\n",
      "15  1834    NY     BC\n",
      "16  1882    NY     ME\n",
      "17  1935    CT     NA\n",
      "18  1983    NY     FA\n",
      "19  1588    NY     FA\n",
      "Number of rows: 20\n",
      "Number of columns: 3\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 11: New Row in AIRLINE.SAMDAT8\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df8 corresponds to SAMDAT8\n",
    "# Creating a new row and appending it to df8 using pd.concat()\n",
    "new_row = pd.DataFrame({'SUPID': ['1588'], 'STATE': ['NY'], 'JOBCAT': ['FA']})\n",
    "df11_samdat8 = pd.concat([df8, new_row], ignore_index=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df11_samdat8)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df11_samdat8.shape[0]}')\n",
    "print(f'Number of columns: {df11_samdat8.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    IDNUM      LASTNAME  FIRSTNAME       Salary\n",
      "1    1017         WELCH     DARIUS   $40,858.00\n",
      "7    1076        VENTER    RANDALL   $66,558.00\n",
      "15   1106     MARSHBURN     JASPER   $89,632.00\n",
      "16   1107      THOMPSON      WAYNE   $89,977.00\n",
      "17   1111        RHODES     JEREMY   $40,586.00\n",
      "24   1118        DENNIS      ROGER  $111,379.00\n",
      "32   1126        KIMANI       ANNE   $40,899.00\n",
      "45   1269        CASTON   FRANKLIN   $41,690.00\n",
      "47   1332    STEPHENSON       ADAM   $42,178.00\n",
      "48   1333      BANADYGA     JUSTIN   $88,606.00\n",
      "49   1347        O'NEAL      BRYAN   $40,079.00\n",
      "51   1352        RIVERS      SIMON   $53,798.00\n",
      "56   1379        MORGAN     ALFRED   $42,264.00\n",
      "58   1385        RAYNOR     MILTON   $43,900.00\n",
      "65   1404         COHEN        LEE   $91,376.00\n",
      "68   1407     GREGORSKI     DANIEL   $68,096.00\n",
      "70   1409       HAVELKA    RAYMOND   $41,551.00\n",
      "71   1410        HARRIS    CHARLES   $84,685.00\n",
      "77   1417       NEWKIRK    WILLIAM   $52,270.00\n",
      "79   1420         ROUSE     JEREMY   $43,071.00\n",
      "87   1428         BRADY  CHRISTINE   $68,767.00\n",
      "98   1439    HASENHAUER  CHRISTINA   $70,736.00\n",
      "101  1442       NEWKIRK     SANDRA   $84,536.00\n",
      "102  1443         WELLS      AGNES   $42,274.00\n",
      "106  1478        NEWTON      JAMES   $84,203.00\n",
      "109  1499      BAREFOOT     JOSEPH   $43,025.00\n",
      "110  1521        PARKER        JAY   $41,526.00\n",
      "111  1545       HERRERO      CLYDE   $66,130.00\n",
      "113  1556    PENNINGTON    MICHAEL   $71,349.00\n",
      "118  1639  CARTER-COHEN      KAREN   $40,260.00\n",
      "125  1739    BRANCACCIO     JOSEPH   $66,517.00\n",
      "126  1777        LUFKIN        ROY  $109,630.00\n",
      "129  1830         TRIPP      KATHY   $84,471.00\n",
      "131  1839        NORRIS      DIANE   $43,433.00\n",
      "134  1882        TUCKER       ALAN   $41,538.00\n",
      "135  1890    STEPHENSON     ROBERT   $91,908.00\n",
      "137  1905        GRAHAM      ALVIN   $65,111.00\n",
      "141  1928      UPCHURCH      LARRY   $89,858.00\n",
      "142  1935     FERNANDEZ    KATRINA   $51,081.00\n",
      "Number of rows: 39\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 13: Employees with salaries over $40,000\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df7 corresponds to SAMDAT7 and df5 to SAMDAT5\n",
    "# Ensuring that the column names are uppercase to match the expected format\n",
    "df7.columns = [x.upper() for x in df7.columns]\n",
    "df5.columns = [x.upper() for x in df5.columns]\n",
    "\n",
    "# Merging dataframes on IDNUM, ensuring both DataFrames have the 'IDNUM' column\n",
    "if 'IDNUM' in df7.columns and 'IDNUM' in df5.columns:\n",
    "    df13_merged = pd.merge(df7[['IDNUM', 'LNAME', 'FNAME']], df5[['IDNUM', 'SALARY']], on='IDNUM')\n",
    "\n",
    "    # Filtering for salaries greater than $40,000\n",
    "    df13_gtforty = df13_merged[df13_merged['SALARY'] > 40000]\n",
    "\n",
    "    # Renaming columns as specified in the SAS code\n",
    "    df13_gtforty = df13_gtforty.rename(columns={'LNAME': 'LASTNAME', 'FNAME': 'FIRSTNAME', 'SALARY': 'Salary'})\n",
    "\n",
    "    # Formatting the salary as dollar values\n",
    "    df13_gtforty['Salary'] = df13_gtforty['Salary'].apply(lambda x: f'${x:,.2f}')\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df13_gtforty)\n",
    "\n",
    "    # Show the number of columns and rows for the data output\n",
    "    print(f'Number of rows: {df13_gtforty.shape[0]}')\n",
    "    print(f'Number of columns: {df13_gtforty.shape[1]}')\n",
    "else:\n",
    "    print(\"Error: 'IDNUM' column is missing in one of the DataFrames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 14        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 14: Number of Passengers per Flight by Date';\n",
    "\n",
    "proc print data=samples.SAMDAT1 noobs;\n",
    "  var DATES BOARDED;\n",
    "  by FLIGHT DEST;\n",
    "  sumby FLIGHT;\n",
    "  sum BOARDED;\n",
    "run;\n",
    "\n",
    "title 'Libname Sample 14: Maximum Number of Passengers per Flight';\n",
    "\n",
    "proc means data=samples.SAMDAT1 fw=5 maxdec=1 max;\n",
    "  var BOARDED;\n",
    "  class FLIGHT;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "First Part: Lists the number of passengers boarded per flight and date, grouping the data by flight and destination. It sums the number of boarded passengers by flight.\n",
    "Second Part: Calculates the maximum number of passengers that boarded any flight, grouped by flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FLIGHT DEST      DATES  BOARDED\n",
      "0     114  LAX 1998-03-01    172.0\n",
      "1     114  LAX 1998-03-02    119.0\n",
      "2     114  LAX 1998-03-03    197.0\n",
      "3     114  LAX 1998-03-04    178.0\n",
      "4     114  LAX 1998-03-05    117.0\n",
      "5     114  LAX 1998-03-06    128.0\n",
      "6     114  LAX 1998-03-07    160.0\n",
      "7     132  YYZ 1998-03-01    115.0\n",
      "8     132  YYZ 1998-03-02    106.0\n",
      "9     132  YYZ 1998-03-03     75.0\n",
      "10    132  YYZ 1998-03-04    117.0\n",
      "11    132  YYZ 1998-03-05    157.0\n",
      "12    132  YYZ 1998-03-06    150.0\n",
      "13    132  YYZ 1998-03-07    164.0\n",
      "14    202  ORD 1998-03-01    151.0\n",
      "15    202  ORD 1998-03-02    120.0\n",
      "16    202  ORD 1998-03-03    118.0\n",
      "17    202  ORD 1998-03-04    148.0\n",
      "18    202  ORD 1998-03-05    104.0\n",
      "19    202  ORD 1998-03-06    115.0\n",
      "20    202  ORD 1998-03-07    175.0\n",
      "21    219  LON 1998-03-01    198.0\n",
      "22    219  LON 1998-03-02    147.0\n",
      "23    219  LON 1998-03-03    197.0\n",
      "24    219  LON 1998-03-04    232.0\n",
      "25    219  LON 1998-03-05    160.0\n",
      "26    219  LON 1998-03-06    163.0\n",
      "27    219  LON 1998-03-07    241.0\n",
      "28    271  PAR 1998-03-01    138.0\n",
      "29    271  PAR 1998-03-02    104.0\n",
      "30    271  PAR 1998-03-03    147.0\n",
      "31    271  PAR 1998-03-04    146.0\n",
      "32    271  PAR 1998-03-05    177.0\n",
      "33    271  PAR 1998-03-07    155.0\n",
      "34    302  WAS 1998-03-01    105.0\n",
      "35    302  WAS 1998-03-02     78.0\n",
      "36    302  WAS 1998-03-03    123.0\n",
      "37    302  WAS 1998-03-04    115.0\n",
      "38    302  WAS 1998-03-06     66.0\n",
      "39    302  WAS 1998-03-07    135.0\n",
      "40    622  FRA 1998-03-01    207.0\n",
      "41    622  FRA 1998-03-02    176.0\n",
      "42    622  FRA 1998-03-03    180.0\n",
      "43    622  FRA 1998-03-04    137.0\n",
      "44    622  FRA 1998-03-05    185.0\n",
      "45    622  FRA 1998-03-07    210.0\n",
      "Number of rows: 46\n",
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 14: Number of Passengers per Flight by Date\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df1 corresponds to SAMDAT1\n",
    "# Grouping data by FLIGHT and DEST, and then aggregating BOARDED\n",
    "df14a_passengers = df1.groupby(['FLIGHT', 'DEST', 'DATES'])['BOARDED'].sum().reset_index()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df14a_passengers)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df14a_passengers.shape[0]}')\n",
    "print(f'Number of columns: {df14a_passengers.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FLIGHT BOARDED\n",
      "0    114   197.0\n",
      "1    132   164.0\n",
      "2    202   175.0\n",
      "3    219   241.0\n",
      "4    271   177.0\n",
      "5    302   135.0\n",
      "6    622   210.0\n",
      "Number of rows: 7\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 14: Maximum Number of Passengers per Flight\n",
    "\n",
    "# Grouping by FLIGHT and finding the maximum number of BOARDED\n",
    "df14b_max_passengers = df1.groupby('FLIGHT')['BOARDED'].max().reset_index()\n",
    "\n",
    "# Formatting the BOARDED column to show one decimal place\n",
    "df14b_max_passengers['BOARDED'] = df14b_max_passengers['BOARDED'].apply(lambda x: f'{x:.1f}')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df14b_max_passengers)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df14b_max_passengers.shape[0]}')\n",
    "print(f'Number of columns: {df14b_max_passengers.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 16       */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 16: Contents of the SAMDAT2 Table';\n",
    "\n",
    "proc contents data=samples.SAMDAT2;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "The SAS proc contents command provides detailed metadata about a dataset (SAMDAT2), such as column names, types, lengths, and other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   FLIGHT    46 non-null     object        \n",
      " 1   DATES     46 non-null     datetime64[ns]\n",
      " 2   ORIG      46 non-null     object        \n",
      " 3   DEST      46 non-null     object        \n",
      " 4   DELAYCAT  46 non-null     object        \n",
      " 5   DESTYPE   46 non-null     object        \n",
      " 6   DELAY     46 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(5)\n",
      "memory usage: 2.6+ KB\n",
      "Number of rows: 46\n",
      "Number of columns: 7\n",
      "Column Data Types:\n",
      "FLIGHT              object\n",
      "DATES       datetime64[ns]\n",
      "ORIG                object\n",
      "DEST                object\n",
      "DELAYCAT            object\n",
      "DESTYPE             object\n",
      "DELAY              float64\n",
      "dtype: object\n",
      "Detailed Memory Usage:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   FLIGHT    46 non-null     object        \n",
      " 1   DATES     46 non-null     datetime64[ns]\n",
      " 2   ORIG      46 non-null     object        \n",
      " 3   DEST      46 non-null     object        \n",
      " 4   DELAYCAT  46 non-null     object        \n",
      " 5   DESTYPE   46 non-null     object        \n",
      " 6   DELAY     46 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(5)\n",
      "memory usage: 15.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 16: Contents of the SAMDAT2 Table\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df2 corresponds to SAMDAT2\n",
    "# Displaying detailed information about df2\n",
    "print(\"DataFrame Info:\")\n",
    "df16_samdat2_info = df2.info()\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df2.shape[0]}')\n",
    "print(f'Number of columns: {df2.shape[1]}')\n",
    "\n",
    "# Detailed data types of each column\n",
    "print(\"Column Data Types:\")\n",
    "print(df2.dtypes)\n",
    "\n",
    "# To check for duplicate columns and merge them (if necessary)\n",
    "df2 = df2.loc[:,~df2.columns.duplicated()]\n",
    "\n",
    "# Showing additional memory usage information\n",
    "print(\"Detailed Memory Usage:\")\n",
    "print(df2.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*==========================*/\n",
    " /* LIBNAME Sample 17        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 17: Ranking of Delayed Flights';\n",
    "\n",
    "options pageno=1;\n",
    "\n",
    "proc rank data=samples.SAMDAT2 descending\n",
    "    ties=low out=work.ranked;\n",
    "  var DELAY;\n",
    "  ranks RANKING;\n",
    "run;\n",
    "\n",
    "proc print data=work.ranked;\n",
    "  format DELAY 2.0;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS program performs the following steps:\n",
    "\n",
    "Ranking: It ranks the flights based on the DELAY column in descending order from the dataset SAMDAT2. The ties=low option indicates that in the event of ties, the lowest rank (i.e., the highest priority) is assigned.\n",
    "Output: It creates a new dataset ranked with the ranking included.\n",
    "Printing: It prints the ranked dataset with the DELAY formatted to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FLIGHT      DATES ORIG DEST      DELAYCAT        DESTYPE  DELAY  RANKING\n",
      "0     114 1998-03-01  LGA  LAX  1-10 Minutes       Domestic    8.0      9.0\n",
      "1     202 1998-03-01  LGA  ORD      No Delay       Domestic   -5.0     42.0\n",
      "2     219 1998-03-01  LGA  LON   11+ Minutes  International   18.0      4.0\n",
      "3     622 1998-03-01  LGA  FRA      No Delay  International   -5.0     42.0\n",
      "4     132 1998-03-01  LGA  YYZ   11+ Minutes  International   14.0      8.0\n",
      "5     271 1998-03-01  LGA  PAR  1-10 Minutes  International    5.0     13.0\n",
      "6     302 1998-03-01  LGA  WAS      No Delay       Domestic   -2.0     36.0\n",
      "7     114 1998-03-02  LGA  LAX      No Delay       Domestic    0.0     28.0\n",
      "8     202 1998-03-02  LGA  ORD  1-10 Minutes       Domestic    5.0     13.0\n",
      "9     219 1998-03-02  LGA  LON   11+ Minutes  International   18.0      4.0\n",
      "10    622 1998-03-02  LGA  FRA      No Delay  International    0.0     28.0\n",
      "11    132 1998-03-02  LGA  YYZ  1-10 Minutes  International    5.0     13.0\n",
      "12    271 1998-03-02  LGA  PAR  1-10 Minutes  International    4.0     19.0\n",
      "13    302 1998-03-02  LGA  WAS      No Delay       Domestic    0.0     28.0\n",
      "14    114 1998-03-03  LGA  LAX      No Delay       Domestic   -1.0     32.0\n",
      "15    202 1998-03-03  LGA  ORD      No Delay       Domestic   -1.0     32.0\n",
      "16    219 1998-03-03  LGA  LON  1-10 Minutes  International    4.0     19.0\n",
      "17    622 1998-03-03  LGA  FRA      No Delay  International   -2.0     36.0\n",
      "18    132 1998-03-03  LGA  YYZ  1-10 Minutes  International    6.0     12.0\n",
      "19    271 1998-03-03  LGA  PAR  1-10 Minutes  International    2.0     25.0\n",
      "20    302 1998-03-03  LGA  WAS  1-10 Minutes       Domestic    5.0     13.0\n",
      "21    114 1998-03-04  LGA  LAX   11+ Minutes       Domestic   15.0      6.0\n",
      "22    202 1998-03-04  LGA  ORD      No Delay       Domestic   -5.0     42.0\n",
      "23    219 1998-03-04  LGA  LON  1-10 Minutes  International    3.0     22.0\n",
      "24    622 1998-03-04  LGA  FRA   11+ Minutes  International   30.0      1.0\n",
      "25    132 1998-03-04  LGA  YYZ      No Delay  International   -5.0     42.0\n",
      "26    271 1998-03-04  LGA  PAR  1-10 Minutes  International    5.0     13.0\n",
      "27    302 1998-03-04  LGA  WAS  1-10 Minutes       Domestic    7.0     10.0\n",
      "28    114 1998-03-05  LGA  LAX      No Delay       Domestic   -2.0     36.0\n",
      "29    202 1998-03-05  LGA  ORD  1-10 Minutes       Domestic    2.0     25.0\n",
      "30    219 1998-03-05  LGA  LON  1-10 Minutes  International    3.0     22.0\n",
      "31    622 1998-03-05  LGA  FRA      No Delay  International   -6.0     46.0\n",
      "32    132 1998-03-05  LGA  YYZ  1-10 Minutes  International    3.0     22.0\n",
      "33    271 1998-03-05  LGA  PAR  1-10 Minutes  International    5.0     13.0\n",
      "34    114 1998-03-06  LGA  LAX      No Delay       Domestic   -1.0     32.0\n",
      "35    202 1998-03-06  LGA  ORD      No Delay       Domestic   -3.0     41.0\n",
      "36    219 1998-03-06  LGA  LON   11+ Minutes  International   27.0      2.0\n",
      "37    132 1998-03-06  LGA  YYZ  1-10 Minutes  International    7.0     10.0\n",
      "38    302 1998-03-06  LGA  WAS  1-10 Minutes       Domestic    1.0     27.0\n",
      "39    114 1998-03-07  LGA  LAX      No Delay       Domestic   -1.0     32.0\n",
      "40    202 1998-03-07  LGA  ORD      No Delay       Domestic   -2.0     36.0\n",
      "41    219 1998-03-07  LGA  LON   11+ Minutes  International   15.0      6.0\n",
      "42    622 1998-03-07  LGA  FRA   11+ Minutes  International   21.0      3.0\n",
      "43    132 1998-03-07  LGA  YYZ      No Delay  International   -2.0     36.0\n",
      "44    271 1998-03-07  LGA  PAR  1-10 Minutes  International    4.0     19.0\n",
      "45    302 1998-03-07  LGA  WAS      No Delay       Domestic    0.0     28.0\n",
      "Number of rows: 46\n",
      "Number of columns: 8\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 17: Ranking of Delayed Flights\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df2 corresponds to SAMDAT2\n",
    "# Ranking the 'DELAY' in descending order, handling ties by assigning the lowest rank to the highest values\n",
    "df17_ranked = df2.copy()\n",
    "df17_ranked['RANKING'] = df17_ranked['DELAY'].rank(method='min', ascending=False)\n",
    "\n",
    "# Formatting the DELAY to show only two decimal places\n",
    "df17_ranked['DELAY'] = df17_ranked['DELAY'].round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df17_ranked)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df17_ranked.shape[0]}')\n",
    "print(f'Number of columns: {df17_ranked.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*==========================*/\n",
    " /* LIBNAME Sample 18        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 18: Number of Employees by Jobcode';\n",
    "\n",
    "proc tabulate data=samples.SAMDAT5 format=3.0;\n",
    "   class JOBCODE;\n",
    "   table JOBCODE*n;\n",
    "   keylabel n=\"#\";\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "This SAS program generates a table that counts the number of entries (employees) for each unique JOBCODE. This is a common task when summarizing categorical data, and proc tabulate is particularly suited for creating complex summary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   JOBCODE  # of Employees\n",
      "0      TA2              20\n",
      "1      FA2              16\n",
      "2      ME2              14\n",
      "3      TA3              12\n",
      "4      FA1              11\n",
      "5      PT2              10\n",
      "6      TA1               9\n",
      "7      BCK               9\n",
      "8      PT1               8\n",
      "9      ME1               8\n",
      "10     SCP               7\n",
      "11     FA3               7\n",
      "12     ME3               7\n",
      "13     NA1               5\n",
      "14     NA2               3\n",
      "15     PT3               2\n",
      "Number of rows: 16\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 18: Number of Employees by Jobcode\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df5 corresponds to SAMDAT5\n",
    "# Counting the number of employees per JOBCODE\n",
    "df18_jobcode_counts = df5['JOBCODE'].value_counts().reset_index()\n",
    "df18_jobcode_counts.columns = ['JOBCODE', '# of Employees']\n",
    "\n",
    "# Display the result\n",
    "print(df18_jobcode_counts)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df18_jobcode_counts.shape[0]}')\n",
    "print(f'Number of columns: {df18_jobcode_counts.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 19        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 19: SAMAT5 After Appending SAMDAT6';\n",
    "\n",
    "proc append base=samples.SAMDAT5\n",
    "            data=samples.SAMDAT6;\n",
    "run;\n",
    "\n",
    "proc print data=samples.SAMDAT5;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "The proc append procedure in SAS is an efficient way to append rows from one dataset (data) to another (base), typically more efficient than concatenating datasets because it appends in place without creating a new dataset unless necessary. After the append operation, the combined data is displayed with proc print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1840\\4024633622.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Assuming DataFrame df5 corresponds to SAMDAT5 and DataFrame df6 to SAMDAT6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Appending df6 to df5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdf19_samdat5_appended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Display the updated DataFrame df5 after appending df6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf19_samdat5_appended\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6292\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6293\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6294\u001b[0m         ):\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6296\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# Libname Sample 19: SAMAT5 After Appending SAMDAT6\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df5 corresponds to SAMDAT5 and DataFrame df6 to SAMDAT6\n",
    "# Appending df6 to df5\n",
    "df19_samdat5_appended = df5.append(df6, ignore_index=True)\n",
    "\n",
    "# Display the updated DataFrame df5 after appending df6\n",
    "print(df19_samdat5_appended)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df19_samdat5_appended.shape[0]}')\n",
    "print(f'Number of columns: {df19_samdat5_appended.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SEX JOBCODE   SALARY      BIRTH      HIRED\n",
      "0     M     TA1  28880.0 1959-03-02 1992-03-26\n",
      "1     M     TA3  40858.0 1957-12-28 1981-10-16\n",
      "2     F     TA3  39392.0 1965-05-19 1984-10-23\n",
      "3     F     TA1  28558.0 1964-04-10 1992-09-13\n",
      "4     F     TA1  26533.0 1969-11-09 1991-11-23\n",
      "..   ..     ...      ...        ...        ...\n",
      "155   M     TA3  36598.0 1961-12-28 1987-03-13\n",
      "156   F     FA1  22123.0 1972-08-07 1992-10-29\n",
      "157   M     TA3  36514.0 1963-11-30 1987-10-07\n",
      "158   F     TA3  42260.0 1957-06-26 1984-01-28\n",
      "159   M     SCP  23100.0 1970-09-10 1992-11-02\n",
      "\n",
      "[160 rows x 5 columns]\n",
      "Number of rows: 160\n",
      "Number of columns: 5\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 19: SAMAT5 After Appending SAMDAT6\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df5 corresponds to SAMDAT5 and DataFrame df6 to SAMDAT6\n",
    "# Appending df6 to df5 using pd.concat() instead of append()\n",
    "df19_samdat5_appended = pd.concat([df5, df6], ignore_index=True)\n",
    "\n",
    "# Display the updated DataFrame df5 after appending df6\n",
    "print(df19_samdat5_appended)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df19_samdat5_appended.shape[0]}')\n",
    "print(f'Number of columns: {df19_samdat5_appended.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 20        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 20: Invoice Frequency by Country';\n",
    "\n",
    "proc freq data=samples.SAMDAT9 (keep=INVNUM COUNTRY);\n",
    "  tables COUNTRY;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "The SAS program: Uses proc freq to analyze and produce a frequency distribution for the COUNTRY column after keeping only the INVNUM and COUNTRY columns from the dataset SAMDAT9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     COUNTRY  FREQUENCY\n",
      "0        USA         10\n",
      "1     Brazil          4\n",
      "2  Argentina          2\n",
      "3  Australia          1\n",
      "Number of rows: 4\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 20: Invoice Frequency by Country\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df9 corresponds to SAMDAT9 and contains columns INVNUM and COUNTRY\n",
    "# Keeping only the relevant columns\n",
    "df20_filtered = df9[['INVNUM', 'COUNTRY']]\n",
    "\n",
    "# Calculating frequency of each country\n",
    "country_frequency = df20_filtered['COUNTRY'].value_counts().reset_index()\n",
    "country_frequency.columns = ['COUNTRY', 'FREQUENCY']\n",
    "\n",
    "# Display the frequency distribution\n",
    "print(country_frequency)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {country_frequency.shape[0]}')\n",
    "print(f'Number of columns: {country_frequency.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " /*==========================*/\n",
    " /* LIBNAME Sample 21        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 21: High Bills--Not Paid';\n",
    "\n",
    "proc sql;\n",
    "  create view work.allinv as\n",
    "  select PAIDON, BILLEDON, INVNUM, AMTINUS, BILLEDTO\n",
    "    from samples.SAMDAT9 (obs=5);\n",
    "quit;\n",
    "\n",
    "data work.notpaid(keep=INVNUM BILLEDTO AMTINUS BILLEDON);\n",
    "\n",
    "  set work.allinv;\n",
    "  where PAIDON is missing and AMTINUS>=300000.00;\n",
    "run;\n",
    "\n",
    "proc print data=work.notpaid label;\n",
    "  format AMTINUS dollar20.2;\n",
    "  label  AMTINUS=amountinus\n",
    "         BILLEDON=billedon\n",
    "         INVNUM=invoicenum\n",
    "         BILLEDTO=billedto;\n",
    "run;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "Creating a View: A view named allinv is created with the first 5 observations from SAMDAT9 that includes selected columns: PAIDON, BILLEDON, INVNUM, AMTINUS, and BILLEDTO.\n",
    "Filtering Data: A new dataset notpaid is created from allinv, keeping only invoices where PAIDON is missing and AMTINUS is at least 300,000.00.\n",
    "Printing the Data: The dataset notpaid is printed with formatted monetary values and custom labels for columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PAIDON   billedon  invoicenum      amountinus  billedto\n",
      "1    NaT 1998-10-05     11271.0  $11,063,836.00  18543489\n",
      "Number of rows: 1\n",
      "Number of columns: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\comma\\AppData\\Local\\Temp\\ipykernel_30200\\4237621202.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df21_notpaid['AMTINUS'] = df21_notpaid['AMTINUS'].apply(lambda x: f'${x:,.2f}')\n",
      "C:\\Users\\comma\\AppData\\Local\\Temp\\ipykernel_30200\\4237621202.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df21_notpaid.rename(columns={'AMTINUS': 'amountinus', 'BILLEDON': 'billedon', 'INVNUM': 'invoicenum', 'BILLEDTO': 'billedto'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 21: High Bills--Not Paid\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df9 corresponds to SAMDAT9\n",
    "# Creating a view (equivalent in pandas)\n",
    "df21_allinv = df9[['PAIDON', 'BILLEDON', 'INVNUM', 'AMTINUS', 'BILLEDTO']].head(5)\n",
    "\n",
    "# Filtering to find not paid high bills\n",
    "df21_notpaid = df21_allinv[df21_allinv['PAIDON'].isna() & (df21_allinv['AMTINUS'] >= 300000)]\n",
    "\n",
    "# Applying formatting and labeling\n",
    "df21_notpaid['AMTINUS'] = df21_notpaid['AMTINUS'].apply(lambda x: f'${x:,.2f}')\n",
    "df21_notpaid.rename(columns={'AMTINUS': 'amountinus', 'BILLEDON': 'billedon', 'INVNUM': 'invoicenum', 'BILLEDTO': 'billedto'}, inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df21_notpaid)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df21_notpaid.shape[0]}')\n",
    "print(f'Number of columns: {df21_notpaid.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INVNUM</th>\n",
       "      <th>BILLEDTO</th>\n",
       "      <th>AMTBILL</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>AMTINUS</th>\n",
       "      <th>BILLEDBY</th>\n",
       "      <th>BILLEDON</th>\n",
       "      <th>PAIDON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11270.0</td>\n",
       "      <td>39045213</td>\n",
       "      <td>8.738601e+09</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>2256870.0</td>\n",
       "      <td>239185.0</td>\n",
       "      <td>1998-10-05</td>\n",
       "      <td>1998-10-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11271.0</td>\n",
       "      <td>18543489</td>\n",
       "      <td>1.106384e+07</td>\n",
       "      <td>USA</td>\n",
       "      <td>11063836.0</td>\n",
       "      <td>457232.0</td>\n",
       "      <td>1998-10-05</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11273.0</td>\n",
       "      <td>19783482</td>\n",
       "      <td>2.521485e+05</td>\n",
       "      <td>USA</td>\n",
       "      <td>252148.5</td>\n",
       "      <td>239185.0</td>\n",
       "      <td>1998-10-06</td>\n",
       "      <td>1998-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11276.0</td>\n",
       "      <td>14324742</td>\n",
       "      <td>1.934460e+06</td>\n",
       "      <td>USA</td>\n",
       "      <td>1934460.0</td>\n",
       "      <td>135673.0</td>\n",
       "      <td>1998-10-06</td>\n",
       "      <td>1998-10-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11278.0</td>\n",
       "      <td>14898029</td>\n",
       "      <td>1.400825e+06</td>\n",
       "      <td>USA</td>\n",
       "      <td>1400825.0</td>\n",
       "      <td>239185.0</td>\n",
       "      <td>1998-10-06</td>\n",
       "      <td>1998-10-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11280.0</td>\n",
       "      <td>39045213</td>\n",
       "      <td>8.738601e+09</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>2256870.0</td>\n",
       "      <td>423286.0</td>\n",
       "      <td>1998-10-07</td>\n",
       "      <td>1998-10-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11282.0</td>\n",
       "      <td>19783482</td>\n",
       "      <td>2.521485e+05</td>\n",
       "      <td>USA</td>\n",
       "      <td>252148.5</td>\n",
       "      <td>457232.0</td>\n",
       "      <td>1998-10-07</td>\n",
       "      <td>1998-10-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11285.0</td>\n",
       "      <td>38763919</td>\n",
       "      <td>2.234301e+06</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>2256870.0</td>\n",
       "      <td>239185.0</td>\n",
       "      <td>1998-10-10</td>\n",
       "      <td>1998-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11286.0</td>\n",
       "      <td>43459747</td>\n",
       "      <td>1.483660e+07</td>\n",
       "      <td>Australia</td>\n",
       "      <td>11063836.0</td>\n",
       "      <td>423286.0</td>\n",
       "      <td>1998-10-10</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11287.0</td>\n",
       "      <td>15432147</td>\n",
       "      <td>2.521485e+05</td>\n",
       "      <td>USA</td>\n",
       "      <td>252148.5</td>\n",
       "      <td>457232.0</td>\n",
       "      <td>1998-10-11</td>\n",
       "      <td>1998-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12051.0</td>\n",
       "      <td>39045213</td>\n",
       "      <td>8.738601e+09</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>2256870.0</td>\n",
       "      <td>457232.0</td>\n",
       "      <td>1998-11-02</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12102.0</td>\n",
       "      <td>18543489</td>\n",
       "      <td>1.106384e+07</td>\n",
       "      <td>USA</td>\n",
       "      <td>11063836.0</td>\n",
       "      <td>239185.0</td>\n",
       "      <td>1998-11-17</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12263.0</td>\n",
       "      <td>19783482</td>\n",
       "      <td>2.521485e+05</td>\n",
       "      <td>USA</td>\n",
       "      <td>252148.5</td>\n",
       "      <td>423286.0</td>\n",
       "      <td>1998-12-05</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12468.0</td>\n",
       "      <td>14898029</td>\n",
       "      <td>1.400825e+06</td>\n",
       "      <td>USA</td>\n",
       "      <td>1400825.0</td>\n",
       "      <td>135673.0</td>\n",
       "      <td>1998-12-24</td>\n",
       "      <td>1999-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12471.0</td>\n",
       "      <td>39045213</td>\n",
       "      <td>8.738601e+09</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>2256870.0</td>\n",
       "      <td>457232.0</td>\n",
       "      <td>1998-12-27</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12476.0</td>\n",
       "      <td>38763919</td>\n",
       "      <td>2.234301e+06</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>2256870.0</td>\n",
       "      <td>135673.0</td>\n",
       "      <td>1998-12-24</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12478.0</td>\n",
       "      <td>15432147</td>\n",
       "      <td>2.521485e+05</td>\n",
       "      <td>USA</td>\n",
       "      <td>252148.5</td>\n",
       "      <td>423286.0</td>\n",
       "      <td>1998-12-24</td>\n",
       "      <td>1999-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     INVNUM  BILLEDTO       AMTBILL    COUNTRY     AMTINUS  BILLEDBY  \\\n",
       "0   11270.0  39045213  8.738601e+09     Brazil   2256870.0  239185.0   \n",
       "1   11271.0  18543489  1.106384e+07        USA  11063836.0  457232.0   \n",
       "2   11273.0  19783482  2.521485e+05        USA    252148.5  239185.0   \n",
       "3   11276.0  14324742  1.934460e+06        USA   1934460.0  135673.0   \n",
       "4   11278.0  14898029  1.400825e+06        USA   1400825.0  239185.0   \n",
       "5   11280.0  39045213  8.738601e+09     Brazil   2256870.0  423286.0   \n",
       "6   11282.0  19783482  2.521485e+05        USA    252148.5  457232.0   \n",
       "7   11285.0  38763919  2.234301e+06  Argentina   2256870.0  239185.0   \n",
       "8   11286.0  43459747  1.483660e+07  Australia  11063836.0  423286.0   \n",
       "9   11287.0  15432147  2.521485e+05        USA    252148.5  457232.0   \n",
       "10  12051.0  39045213  8.738601e+09     Brazil   2256870.0  457232.0   \n",
       "11  12102.0  18543489  1.106384e+07        USA  11063836.0  239185.0   \n",
       "12  12263.0  19783482  2.521485e+05        USA    252148.5  423286.0   \n",
       "13  12468.0  14898029  1.400825e+06        USA   1400825.0  135673.0   \n",
       "14  12471.0  39045213  8.738601e+09     Brazil   2256870.0  457232.0   \n",
       "15  12476.0  38763919  2.234301e+06  Argentina   2256870.0  135673.0   \n",
       "16  12478.0  15432147  2.521485e+05        USA    252148.5  423286.0   \n",
       "\n",
       "     BILLEDON     PAIDON  \n",
       "0  1998-10-05 1998-10-18  \n",
       "1  1998-10-05        NaT  \n",
       "2  1998-10-06 1998-11-11  \n",
       "3  1998-10-06 1998-10-20  \n",
       "4  1998-10-06 1998-10-19  \n",
       "5  1998-10-07 1998-10-20  \n",
       "6  1998-10-07 1998-10-25  \n",
       "7  1998-10-10 1998-11-30  \n",
       "8  1998-10-10        NaT  \n",
       "9  1998-10-11 1998-11-04  \n",
       "10 1998-11-02        NaT  \n",
       "11 1998-11-17        NaT  \n",
       "12 1998-12-05        NaT  \n",
       "13 1998-12-24 1999-01-02  \n",
       "14 1998-12-27        NaT  \n",
       "15 1998-12-24        NaT  \n",
       "16 1998-12-24 1999-01-02  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*==========================*/\n",
    " /* LIBNAME Sample 22        */\n",
    " /*==========================*/\n",
    "\n",
    "title 'Libname Sample 22: Interns Who Are Family Members of Employees';\n",
    "\n",
    "\n",
    "proc sql;\n",
    "  create view emp_csr as\n",
    "  select * from samples.SAMDAT10\n",
    "    where dept in ('CSR010', 'CSR011', 'CSR004');\n",
    "\n",
    "  select samdat13.LASTNAME, samdat13.FIRSTNAM, samdat13.EMPID,\n",
    "         samdat13.FAMILYID, samdat13.GENDER,\n",
    "         samdat13.DEPT, samdat13.HIREDATE\n",
    "    from emp_csr, samples.samdat13\n",
    "    where emp_csr.EMPID=samdat13.FAMILYID;\n",
    "quit;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAS Code Summary\n",
    "View Creation: A view named emp_csr is created from SAMDAT10 to include only employees from certain departments.\n",
    "Data Selection: Data is selected from another dataset (SAMDAT13) based on a join condition that matches the family IDs of the interns to the employee IDs in the view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMPID</th>\n",
       "      <th>HIREDATE</th>\n",
       "      <th>DEPT</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>LASTNAME</th>\n",
       "      <th>FIRSTNAM</th>\n",
       "      <th>MIDDLENA</th>\n",
       "      <th>FAMILYID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>765111.0</td>\n",
       "      <td>1998-05-04</td>\n",
       "      <td>CSR011</td>\n",
       "      <td>M</td>\n",
       "      <td>NISHIMATSU-LYNCH</td>\n",
       "      <td>RICHARD</td>\n",
       "      <td>ITO</td>\n",
       "      <td>677890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>765112.0</td>\n",
       "      <td>1998-05-04</td>\n",
       "      <td>CSR010</td>\n",
       "      <td>M</td>\n",
       "      <td>SMITH</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>MICHAEL</td>\n",
       "      <td>234967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>219776.0</td>\n",
       "      <td>1998-04-15</td>\n",
       "      <td>ACC024</td>\n",
       "      <td>F</td>\n",
       "      <td>PASTORELLI</td>\n",
       "      <td>ZORA</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>245233.0</td>\n",
       "      <td>1998-04-10</td>\n",
       "      <td>ACC013</td>\n",
       "      <td>None</td>\n",
       "      <td>ALI</td>\n",
       "      <td>SADIQ</td>\n",
       "      <td>H.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>245234.0</td>\n",
       "      <td>1998-04-10</td>\n",
       "      <td>ACC024</td>\n",
       "      <td>F</td>\n",
       "      <td>MEHAILESCU</td>\n",
       "      <td>NADIA</td>\n",
       "      <td>P.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>326721.0</td>\n",
       "      <td>1998-05-01</td>\n",
       "      <td>SHP002</td>\n",
       "      <td>M</td>\n",
       "      <td>CALHOUN</td>\n",
       "      <td>WILLIS</td>\n",
       "      <td>BEAUREGARD</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      EMPID   HIREDATE    DEPT GENDER          LASTNAME FIRSTNAM    MIDDLENA  \\\n",
       "0  765111.0 1998-05-04  CSR011      M  NISHIMATSU-LYNCH  RICHARD         ITO   \n",
       "1  765112.0 1998-05-04  CSR010      M             SMITH   ROBERT     MICHAEL   \n",
       "2  219776.0 1998-04-15  ACC024      F        PASTORELLI     ZORA        None   \n",
       "3  245233.0 1998-04-10  ACC013   None               ALI    SADIQ          H.   \n",
       "4  245234.0 1998-04-10  ACC024      F        MEHAILESCU    NADIA          P.   \n",
       "5  326721.0 1998-05-01  SHP002      M           CALHOUN   WILLIS  BEAUREGARD   \n",
       "\n",
       "   FAMILYID  \n",
       "0  677890.0  \n",
       "1  234967.0  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "5       NaN  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['LASTNAME', 'EMPID', 'GENDER', 'DEPT', 'HIREDATE'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m df22_family_interns \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df22_emp_csr, df13, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMPID\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAMILYID\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Selecting the desired columns\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df22_family_interns \u001b[38;5;241m=\u001b[39m \u001b[43mdf22_family_interns\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLASTNAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFIRSTNAM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEMPID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFAMILYID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGENDER\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDEPT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHIREDATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(df22_family_interns)\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\comma\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['LASTNAME', 'EMPID', 'GENDER', 'DEPT', 'HIREDATE'] not in index\""
     ]
    }
   ],
   "source": [
    "# Libname Sample 22: Interns Who Are Family Members of Employees\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df10 corresponds to SAMDAT10 and DataFrame df13 to SAMDAT13\n",
    "# Creating the view equivalent by filtering df10 for specific departments\n",
    "df22_emp_csr = df10[df10['DEPT'].isin(['CSR010', 'CSR011', 'CSR004'])]\n",
    "\n",
    "# Joining df22_emp_csr with df13 on EMPID matching FAMILYID\n",
    "df22_family_interns = pd.merge(df22_emp_csr, df13, left_on='EMPID', right_on='FAMILYID')\n",
    "\n",
    "# Selecting the desired columns\n",
    "df22_family_interns = df22_family_interns[['LASTNAME', 'FIRSTNAM', 'EMPID', 'FAMILYID', 'GENDER', 'DEPT', 'HIREDATE']]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df22_family_interns)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df22_family_interns.shape[0]}')\n",
    "print(f'Number of columns: {df22_family_interns.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMPID_x</th>\n",
       "      <th>HIREDATE_x</th>\n",
       "      <th>SALARY</th>\n",
       "      <th>DEPT_x</th>\n",
       "      <th>JOBCODE</th>\n",
       "      <th>GENDER_x</th>\n",
       "      <th>BIRTHDTE</th>\n",
       "      <th>LASTNAME_x</th>\n",
       "      <th>FRSTNAME</th>\n",
       "      <th>MIDNAME</th>\n",
       "      <th>PHONE</th>\n",
       "      <th>EMPID_y</th>\n",
       "      <th>HIREDATE_y</th>\n",
       "      <th>DEPT_y</th>\n",
       "      <th>GENDER_y</th>\n",
       "      <th>LASTNAME_y</th>\n",
       "      <th>FIRSTNAM</th>\n",
       "      <th>MIDDLENA</th>\n",
       "      <th>FAMILYID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>234967.0</td>\n",
       "      <td>1993-12-19</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>CSR004</td>\n",
       "      <td>602.0</td>\n",
       "      <td>M</td>\n",
       "      <td>1972-12-21</td>\n",
       "      <td>SMITH</td>\n",
       "      <td>GILBERT</td>\n",
       "      <td>IRVINE</td>\n",
       "      <td>7274</td>\n",
       "      <td>765112.0</td>\n",
       "      <td>1998-05-04</td>\n",
       "      <td>CSR010</td>\n",
       "      <td>M</td>\n",
       "      <td>SMITH</td>\n",
       "      <td>ROBERT</td>\n",
       "      <td>MICHAEL</td>\n",
       "      <td>234967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>677890.0</td>\n",
       "      <td>1993-12-12</td>\n",
       "      <td>37610.0</td>\n",
       "      <td>CSR010</td>\n",
       "      <td>204.0</td>\n",
       "      <td>F</td>\n",
       "      <td>1970-04-24</td>\n",
       "      <td>NISHIMATSU-LYNCH</td>\n",
       "      <td>CAROL</td>\n",
       "      <td>ANNE</td>\n",
       "      <td>6245</td>\n",
       "      <td>765111.0</td>\n",
       "      <td>1998-05-04</td>\n",
       "      <td>CSR011</td>\n",
       "      <td>M</td>\n",
       "      <td>NISHIMATSU-LYNCH</td>\n",
       "      <td>RICHARD</td>\n",
       "      <td>ITO</td>\n",
       "      <td>677890.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    EMPID_x HIREDATE_x   SALARY  DEPT_x  JOBCODE GENDER_x   BIRTHDTE  \\\n",
       "0  234967.0 1993-12-19  17000.0  CSR004    602.0        M 1972-12-21   \n",
       "1  677890.0 1993-12-12  37610.0  CSR010    204.0        F 1970-04-24   \n",
       "\n",
       "         LASTNAME_x FRSTNAME MIDNAME PHONE   EMPID_y HIREDATE_y  DEPT_y  \\\n",
       "0             SMITH  GILBERT  IRVINE  7274  765112.0 1998-05-04  CSR010   \n",
       "1  NISHIMATSU-LYNCH    CAROL    ANNE  6245  765111.0 1998-05-04  CSR011   \n",
       "\n",
       "  GENDER_y        LASTNAME_y FIRSTNAM MIDDLENA  FAMILYID  \n",
       "0        M             SMITH   ROBERT  MICHAEL  234967.0  \n",
       "1        M  NISHIMATSU-LYNCH  RICHARD      ITO  677890.0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df22_family_interns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FIRSTNAM  FAMILYID\n",
      "0   ROBERT  234967.0\n",
      "1  RICHARD  677890.0\n",
      "Number of rows: 2\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "# Libname Sample 22: Interns Who Are Family Members of Employees\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming DataFrame df10 corresponds to SAMDAT10 and DataFrame df13 to SAMDAT13\n",
    "# Creating the view equivalent by filtering df10 for specific departments\n",
    "df22_emp_csr = df10[df10['DEPT'].isin(['CSR010', 'CSR011', 'CSR004'])]\n",
    "\n",
    "# Ensuring that column names in both DataFrames are uppercase\n",
    "df22_emp_csr.columns = [col.upper() for col in df22_emp_csr.columns]\n",
    "df13.columns = [col.upper() for col in df13.columns]\n",
    "\n",
    "# Joining df22_emp_csr with df13 on EMPID matching FAMILYID\n",
    "df22_family_interns = pd.merge(df22_emp_csr, df13, left_on='EMPID', right_on='FAMILYID')\n",
    "\n",
    "# Selecting the desired columns, ensuring they exist in the merged DataFrame\n",
    "columns_to_select = ['LASTNAME', 'FIRSTNAM', 'EMPID', 'FAMILYID', 'GENDER', 'DEPT', 'HIREDATE']\n",
    "df22_family_interns = df22_family_interns[[col for col in columns_to_select if col in df22_family_interns.columns]]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df22_family_interns)\n",
    "\n",
    "# Show the number of columns and rows for the data output\n",
    "print(f'Number of rows: {df22_family_interns.shape[0]}')\n",
    "print(f'Number of columns: {df22_family_interns.shape[1]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
